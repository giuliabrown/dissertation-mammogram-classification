{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Grad-CAM Heatmaps Across Models\n",
    "\n",
    "This notebook is used to compare how different trained models (DenseNet-121, DenseNet-169, MobileNetV3, EfficientNet-B0) visualise regions of interest in mammogram images using Grad-CAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import densenet169, densenet121, mobilenet_v3_large, mobilenet_v3_small, efficientnet_b0\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import PIL\n",
    "\n",
    "from torch.utils.data import Dataset, ConcatDataset, DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalise(img_tensor, mean, std):\n",
    "    \"\"\"\n",
    "    Reverses ImageNet normalisation to recover original image values.\n",
    "    \"\"\"\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    return img_tensor * std + mean\n",
    "\n",
    "# ImageNet mean/std\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM hook setup\n",
    "gradients = None\n",
    "activations = None\n",
    "\n",
    "# Function to register Grad-CAM hooks\n",
    "def register_gradcam_hooks(last_conv_layer):\n",
    "    \"\"\"\n",
    "    Registers forward and backward hooks on the last convolutional layer\n",
    "    to capture activations and gradients needed for Grad-CAM.\n",
    "    \"\"\"\n",
    "    global gradients, activations\n",
    "\n",
    "    def forward_hook(module, args, output):\n",
    "        global activations\n",
    "        activations = output\n",
    "\n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        global gradients\n",
    "        gradients = grad_output\n",
    "\n",
    "    last_conv_layer.register_full_backward_hook(backward_hook, prepend=False)\n",
    "    last_conv_layer.register_forward_hook(forward_hook, prepend=False)\n",
    " \n",
    "def generate_gradcam(model, image):\n",
    "    \"\"\"\n",
    "    Generates Grad-CAM heatmap for a given image.\n",
    "    \"\"\"\n",
    "    global gradients, activations\n",
    "    \n",
    "    model.zero_grad()  # Clear previous gradients\n",
    "    output = model(image)\n",
    "    prob = output.sigmoid()\n",
    "    pred_label = (prob > 0.5).float()\n",
    "    \n",
    "    # Backward pass to get gradients\n",
    "    output.backward(torch.ones_like(output))  \n",
    "    \n",
    "    # Pool gradients across the channels\n",
    "    pooled_gradients = torch.mean(gradients[0], dim=[0, 2, 3])\n",
    "\n",
    "    # Weight the channels by corresponding gradients\n",
    "    for i in range(activations.size()[1]):\n",
    "        activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "    # Compute heatmap\n",
    "    heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "    heatmap = F.relu(heatmap)  # Apply ReLU\n",
    "    heatmap /= torch.max(heatmap)  # Normalize\n",
    "\n",
    "    return heatmap.detach().cpu()\n",
    "\n",
    "def overlay_heatmap(img_tensor, heatmap):\n",
    "    \"\"\"\n",
    "    Overlays the Grad-CAM heatmap on the original image.\n",
    "    \"\"\"\n",
    "\n",
    "    unnorm_img = unnormalise(img_tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    original_img = to_pil_image(unnorm_img.clamp(0, 1), mode='RGB')  # Convert tensor to PIL image\n",
    "\n",
    "    # Resize the heatmap to match image size\n",
    "    overlay = to_pil_image(heatmap, mode='F').resize((224, 224), resample=PIL.Image.BICUBIC)\n",
    "\n",
    "    # Apply colormap\n",
    "    cmap = colormaps['jet']\n",
    "    overlay = (255 * cmap(np.asarray(overlay) ** 2)[:, :, :3]).astype(np.uint8)\n",
    "\n",
    "    return original_img, overlay  # Return original image and overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_d169(model_path):\n",
    "    \"\"\"\n",
    "    Loads DenseNet-169 with custom classifier for binary classification,\n",
    "    and attaches Grad-CAM hooks to its last convolutional layer.\n",
    "    \"\"\"\n",
    "    model = densenet169(weights=None)\n",
    "    model.classifier = nn.Sequential(\n",
    "        #nn.Dropout(0.5),\n",
    "        nn.Linear(in_features=1664, out_features=1)\n",
    "    )\n",
    "    # Load model state\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True, map_location=torch.device(\"cpu\")))\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Register Grad-CAM hooks\n",
    "    last_conv_layer = model.features.denseblock4.denselayer32.conv2\n",
    "    register_gradcam_hooks(last_conv_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_d121(model_path):\n",
    "    \"\"\"\n",
    "    Loads DenseNet-121 with custom classifier for binary classification,\n",
    "    and attaches Grad-CAM hooks to its last convolutional layer.\n",
    "    \"\"\"\n",
    "    model = densenet121(weights=None)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(in_features=1024, out_features=1)\n",
    "    )\n",
    "    # Load model state\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True, map_location=torch.device(\"cpu\")))\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Register Grad-CAM hooks\n",
    "    last_conv_layer = model.features.denseblock4.denselayer16.conv2\n",
    "    register_gradcam_hooks(last_conv_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load model (EfficientNet B0)\n",
    "def load_model_eb0(model_path):\n",
    "    \"\"\"\n",
    "    Loads EffcientNet B0 with custom classifier for binary classification,\n",
    "    and attaches Grad-CAM hooks to its last convolutional layer.\n",
    "    \"\"\"\n",
    "    model = efficientnet_b0(weights=None)\n",
    "    # Modify the classifier for binary classification\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(in_features=1280, out_features=1)\n",
    "    )\n",
    "    # Load model state\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True, map_location=torch.device(\"cpu\")))\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Register Grad-CAM hooks\n",
    "    last_conv_layer = model.features[-1][0]\n",
    "    register_gradcam_hooks(last_conv_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_mnl(model_path):\n",
    "    \"\"\"\n",
    "    Loads MobileNetV3 Large with custom classifier for binary classification,\n",
    "    and attaches Grad-CAM hooks to its last convolutional layer.\n",
    "    \"\"\"\n",
    "    model = mobilenet_v3_large(weights=None)\n",
    "    \n",
    "    # Check the number of input features for the classifier\n",
    "    num_features = model.classifier[0].in_features\n",
    "\n",
    "    # Modify the classifier for binary classification\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(num_features, 1)\n",
    "    )\n",
    "\n",
    "    # Load model state\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True, map_location=torch.device(\"cpu\")))\n",
    "\n",
    "    # Move model to device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Register Grad-CAM hooks on the last convolutional layer\n",
    "    last_conv_layer = model.features[-1][0]\n",
    "    register_gradcam_hooks(last_conv_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load model (MobileNetV3 small)\n",
    "def load_model_mns(model_path):\n",
    "    \"\"\"\n",
    "    Loads MobileNetV3 Small with custom classifier for binary classification,\n",
    "    and attaches Grad-CAM hooks to its last convolutional layer.\n",
    "    \"\"\"\n",
    "    model = mobilenet_v3_small(weights=None)\n",
    "    \n",
    "    # Check the number of input features for the classifier\n",
    "    num_features = model.classifier[0].in_features\n",
    "    # Modify the classifier for binary classification\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.5), \n",
    "        nn.Linear(num_features, 1)\n",
    "    )\n",
    "\n",
    "    # Load model state\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True, map_location=torch.device(\"cpu\")))\n",
    "\n",
    "    # Move model to device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Register Grad-CAM hooks on the last convolutional layer\n",
    "    last_conv_layer = model.features[-1][0]\n",
    "    register_gradcam_hooks(last_conv_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform  = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
    "        image_rgb = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.img_labels.iloc[idx, 2]\n",
    "        \n",
    "        if self.transform:\n",
    "            image_rgb = self.transform(image_rgb)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image_rgb, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cropped calcification data\n",
    "calc_test_label_dir = \"/Users/giulia/Desktop/dissertation-mammogram-classification/mammogram-ai-project/Data/Data png cropped/Calc-Test-png-cropped/labels/calc-test_labels.csv\"\n",
    "calc_test_img_dir = \"/Users/giulia/Desktop/dissertation-mammogram-classification/mammogram-ai-project/Data/Data png cropped/Calc-Test-png-cropped/images\"\n",
    "\n",
    "# now apply the transformations to the calcification images\n",
    "calc_test_data = CustomImageDataset(calc_test_label_dir, calc_test_img_dir, test_transform)\n",
    "\n",
    "# cropped mass data\n",
    "mass_test_label_dir = \"/Users/giulia/Desktop/dissertation-mammogram-classification/mammogram-ai-project/Data/Data png cropped/Mass-Test-png-cropped/labels/mass-test_labels.csv\"\n",
    "mass_test_img_dir = \"/Users/giulia/Desktop/dissertation-mammogram-classification/mammogram-ai-project/Data/Data png cropped/Mass-Test-png-cropped/images\"\n",
    "\n",
    "# now apply the transformations to the mass images\n",
    "mass_test_data = CustomImageDataset(mass_test_label_dir, mass_test_img_dir, test_transform)\n",
    "\n",
    "# Merge test datasets\n",
    "cropped_combined_test_data = ConcatDataset([calc_test_data, mass_test_data])\n",
    "print(f\"Total testing samples: {len(cropped_combined_test_data)}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32  # Adjust based on your GPU memory\n",
    "num_workers = 0\n",
    "\n",
    "cropped_test_dataloader = DataLoader(cropped_combined_test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"Benign\",\n",
    "    1: \"Malignant\",\n",
    "}\n",
    "\n",
    "#mass_sample_idxs = []\n",
    "#sample_idxs = []\n",
    "#sample_idxs = [438, 587, 270, 484, 614, 475, 498, 487, 112, 185, 21, 517, 113, 459, 282]\n",
    "#mass_sample_idxs = [38, 292, 24, 69, 140, 97, 170, 63, 119, 4, 183, 276, 176, 46, 247]\n",
    "mass_sample_idxs = [38, 292, 24, 69, 140, 97, 170, 63, 119]\n",
    "\n",
    "figure = plt.figure(figsize=(9, 9))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    #sample_idx = torch.randint(len(cropped_combined_test_data), size=(1,)).item()\n",
    "    #sample_idxs.append(sample_idx)\n",
    "    #sample_idx = sample_idxs[i-1]\n",
    "    #img, label = cropped_combined_test_data[sample_idx]\n",
    "    #print(img.shape)\n",
    "\n",
    "    #mass_sample_idx = torch.randint(len(mass_test_data), size=(1,)).item()\n",
    "    #mass_sample_idxs.append(mass_sample_idx)\n",
    "    mass_sample_idx = mass_sample_idxs[i-1]\n",
    "    img, label = mass_test_data[mass_sample_idx]\n",
    "\n",
    "    image_np = np.array(img)  # Convert to NumPy array\n",
    "    print(image_np.shape)\n",
    "\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    #plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    if(torch.is_tensor(img)):\n",
    "        plt.imshow(img.permute(1, 2, 0))\n",
    "    else:\n",
    "        plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_visualise_single_image(model, dataset, index=None):\n",
    "    \"\"\"\n",
    "    Displays a single image (original, heatmap, overlay) from the dataset \n",
    "    along with model predictions.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): Trained model for prediction.\n",
    "        dataset (torch.utils.data.Dataset): Dataset containing images.\n",
    "        index (int, optional): The specific index of the image to visualise.\n",
    "                               If None, a random index is chosen.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # If no index is specified, pick a random index from the dataset\n",
    "    if index is None:\n",
    "        index = random.randint(0, len(dataset) - 1)\n",
    "\n",
    "    # Load and prepare the single image\n",
    "    img_tensor, label = dataset[index]\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "    # Get model prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        prob = output.sigmoid()\n",
    "        pred_label = (prob > 0.5).float().item()\n",
    "    \n",
    "    # Compute adjusted confidence score\n",
    "    prob_value = prob.item()\n",
    "    if pred_label == 1:\n",
    "        confidence = (prob_value - 0.5) * 200\n",
    "    else:\n",
    "        confidence = (0.5 - prob_value) * 200\n",
    "\n",
    "    # Generate Grad-CAM heatmap\n",
    "    with torch.set_grad_enabled(True):\n",
    "        heatmap = generate_gradcam(model, img_tensor)\n",
    "\n",
    "    # Convert tensors to displayable images\n",
    "    # overlay_heatmap returns (original_img, overlay_img)\n",
    "    original_img, overlay_img = overlay_heatmap(img_tensor.squeeze(), heatmap)\n",
    "\n",
    "    # Get labels\n",
    "    true_label = \"Malignant\" if label == 1 else \"Benign\"\n",
    "    pred_label_str = \"Malignant\" if pred_label == 1 else \"Benign\"\n",
    "\n",
    "    # Create a figure with 3 columns for original, heatmap, and overlay\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # 1. Original Image\n",
    "    axes[0].imshow(original_img)\n",
    "    axes[0].set_title(\n",
    "        f\"Index: {index}\\nTrue: {true_label}\\nPred: {pred_label_str}\\nConf: {confidence:.1f}%\",\n",
    "        fontsize=11\n",
    "    )\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # 2. Heatmap (Grad-CAM)\n",
    "    axes[1].imshow(heatmap)\n",
    "    axes[1].set_title(\"Grad-CAM Heatmap\", fontsize=11)\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # 3. Overlay (Original + Heatmap)\n",
    "    axes[2].imshow(original_img)\n",
    "    axes[2].imshow(overlay_img, alpha=0.4)\n",
    "    axes[2].set_title(\"Overlay\", fontsize=11)\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_visualise(model, dataset, sample_indices):\n",
    "    \"\"\"\n",
    "    Predicts and visualises results for selected samples using Grad-CAM.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): Trained model for prediction.\n",
    "        dataset (torch.utils.data.Dataset): Dataset to sample from.\n",
    "        sample_indices (List[int]): Indices of samples to visualise.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    sample_images = []\n",
    "    sample_heatmaps = []\n",
    "    sample_labels = []\n",
    "    sample_preds = []\n",
    "    sample_confidences = []\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        X, label = dataset[idx]\n",
    "        X = X.unsqueeze(0).to(device)\n",
    "\n",
    "        # Model prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(X)\n",
    "            prob = output.sigmoid()\n",
    "            pred_label = (prob > 0.5).float().item()\n",
    "\n",
    "        # Confidence score\n",
    "        prob_value = prob.item()\n",
    "        if pred_label == 1:\n",
    "            confidence = (prob_value - 0.5) * 200\n",
    "        else:\n",
    "            confidence = (0.5 - prob_value) * 200\n",
    "\n",
    "        # Grad-CAM\n",
    "        with torch.set_grad_enabled(True):\n",
    "            heatmap = generate_gradcam(model, X)\n",
    "\n",
    "        # Unnormalise image and overlay heatmap\n",
    "        original_img, overlay_img = overlay_heatmap(X.squeeze(), heatmap)\n",
    "\n",
    "        # Store everything\n",
    "        sample_images.append(original_img)\n",
    "        sample_heatmaps.append(overlay_img)\n",
    "        sample_labels.append(int(label))\n",
    "        sample_preds.append(int(pred_label))\n",
    "        sample_confidences.append(confidence)\n",
    "\n",
    "    # Plots\n",
    "    num_examples = len(sample_indices)\n",
    "    num_cols = 3\n",
    "    num_rows = (num_examples + num_cols - 1) // num_cols\n",
    "\n",
    "    # Plot original images and Grad-CAM heatmaps\n",
    "    fig, axes = plt.subplots(num_rows * 2, 3, figsize=(12, 6 * num_rows))\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        row = (i // num_cols) * 2\n",
    "        col = i % num_cols\n",
    "\n",
    "        # Original image\n",
    "        axes[row, col].imshow(sample_images[i], cmap=\"gray\")\n",
    "        true_label = \"Malignant\" if sample_labels[i] == 1 else \"Benign\"\n",
    "        pred_label_str = \"Malignant\" if sample_preds[i] == 1 else \"Benign\"\n",
    "        confidence = sample_confidences[i]\n",
    "        axes[row, col].set_title(f\"True: {true_label}\\nPred: {pred_label_str}\\nConf: {confidence:.1f}%\")\n",
    "        axes[row, col].axis(\"off\")\n",
    "\n",
    "        # Grad-CAM overlay\n",
    "        axes[row + 1, col].imshow(sample_images[i], cmap=\"gray\")\n",
    "        axes[row + 1, col].imshow(sample_heatmaps[i], alpha=0.4, interpolation=\"nearest\")\n",
    "        axes[row + 1, col].set_title(\"Grad-CAM\")\n",
    "        axes[row + 1, col].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#densenet169\n",
    "model_path_run25 = \"/Users/giulia/Desktop/dissertation-mammogram-classification/mammogram-ai-project/webapp/static/models/best_model_run25.pth\"\n",
    "#densenet121\n",
    "model_path_run13 = \"/Users/giulia/Desktop/dissertation-mammogram-classification/mammogram-ai-project/webapp/static/models/best_model_run13.pth\"\n",
    "#mobilenetv3 large\n",
    "model_path_run39 = \"/Users/giulia/Desktop/dissertation-mammogram-classification/mammogram-ai-project/webapp/static/models/best_model_run39.pth\"\n",
    "#mobilenetv3 small\n",
    "model_path_run15 = \"/Users/giulia/Desktop/dissertation-mammogram-classification/mammogram-ai-project/webapp/static/models/best_model_run15.pth\"\n",
    "#efficientnetb0\n",
    "model_path_run20 = \"/Users/giulia/Desktop/dissertation-mammogram-classification/mammogram-ai-project/webapp/static/models/best_model_run20.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d169 = load_model_d169(model_path_run25)\n",
    "model_d121 = load_model_d121(model_path_run13)\n",
    "model_mnl = load_model_mnl(model_path_run39)\n",
    "model_mns = load_model_mns(model_path_run15)\n",
    "model_eb0 = load_model_eb0(model_path_run20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_visualise_single_image(model_d121, mass_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_visualise(model_d121, cropped_combined_test_data, mass_sample_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
